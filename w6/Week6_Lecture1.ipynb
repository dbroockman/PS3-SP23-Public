{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POLSCI 3\n",
    "\n",
    "# Week 6, Lecture Notebook 1: Quantifying Uncertainty\n",
    "\n",
    "This week, we'll learn about **quantifying uncertainty** in experiments.\n",
    "\n",
    "So far, we've just looked at differences between groups in experiments and assumed that the differences reflect a true causal effect.\n",
    "\n",
    "For example, in Week 5, Lecture 2, we found that voters assigned to the Hawthorne condition voted at a rate only 0.3 percentage points higher than those assigned to the Civic Duty condition. We therefore concluded that the Hawthorne condition had a tiny effect on turnout over and above the Civic Duty condition; i.e., that there was a small but *real* effect of telling people they were being studied on voter turnout.\n",
    "\n",
    "But what if the Hawthorne condition didn't actually cause *anyone* additional to vote that wouldn't have voted if they got the Civic Duty mailer? Is it possible that *even if* the Civic Duty mailer and Hawthorne mailer had the exact same effect, that we might see those assigned to the Hawthorne condition voting at slightly higher rates anyway? Yes.\n",
    "\n",
    "Put another way, if the true effect of a treatment relative to a control group is 0, would we always see exactly 0 difference between the average outcomes in the treatment and control groups in an experiment? *No.* It's possible that we might, just by chance, randomize people with slightly lower/higher values of the outcome to one group in an experiment.\n",
    "\n",
    "For example, it's possible that we might, just by chance, randomize people that were a little more likely to vote anyway into the Hawthorne group---making it look like the Hawthorne mailing has a slightly higher effect than the Civic Duty mailing, even if the effects are the same.\n",
    "\n",
    "The good news is that we can figure out how likely this is.\n",
    "\n",
    "## Bias vs. Noise\n",
    "\n",
    "Wait, doesn't this mean experiments are no different than observational studies, if the answers they give us aren't guaranteed to be exactly right?\n",
    "\n",
    "No. To understand why not, we need to understand the difference between **bias** and **noise**.\n",
    "\n",
    "The Potential Outcomes framework showed us that, in any experiment, there is a right answer: the **True Average Treatment Effect**. For example, if we could see all the potential outcomes in the social pressure experiment from last week, we would know for sure what exactly the effect of the Hawthorne and Civic Duty mail was on *everyone*, and so we could tell what the true effect is. Real studies never give us the exact right answer, for two reasons:\n",
    "\n",
    "### Bias\n",
    "\n",
    "The first reason is bias, which means when *we expect* the answer that a study will give us will be *systematically different* from correct *in a consistent direction*.\n",
    "\n",
    "For example, we talked in previous weeks about *omitted variable bias* when we saw the dataset on happiness across countries. For example, looking at the relationship between happiness and democracy might give us a *biased* answer about the causal effect of democracy on happiness because income causes both. If you and your friend both collected data on 10 countries, we'd expect to see this same bias in both sets of countries.\n",
    "\n",
    "In other words, **we know we're looking at bias if, if we were to do the same study again, we'd keep getting results that were different from the truth in the same way**.\n",
    "\n",
    "For example, an archer who has a bias towards the upper left part of a target would land arrows like this:\n",
    "\n",
    "<img src=\"highbias_lownoise.png\" width=\"20%\" height=\"auto\">\n",
    "\n",
    "In this metaphor, each of these dots is a study. If a bias effects a study design, its results are going to be *consistently wrong in one direction*.\n",
    "\n",
    "Experiments do not have bias.\n",
    "\n",
    "### Noise\n",
    "\n",
    "Noise is different. Experiments do have noise.\n",
    "\n",
    "Intuitively, if we randomize two groups, we'll see a difference between them, even if there is no effect of whatever we give one group. In fact, I could just randomize a dataset on my computer into two pieces, and I'd expect to see some differences between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(estimatr); set.seed(1027)\n",
    "\n",
    "example.data <- data.frame(outcome = seq(from=0,to=1,length.out=10000),\n",
    "                           treat = sample(c(0,1), 10000, replace = TRUE))\n",
    "head(example.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_means(outcome ~ treat, example.data, condition1 = 0, condition2 = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.data <- data.frame(outcome = seq(from=0,to=1,length.out=10000),\n",
    "                           treat = sample(c(0,1), 10000, replace = TRUE))\n",
    "difference_in_means(outcome ~ treat, example.data, condition1 = 0, condition2 = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.data <- data.frame(outcome = seq(from=0,to=1,length.out=10000),\n",
    "                           treat = sample(c(0,1), 10000, replace = TRUE))\n",
    "difference_in_means(outcome ~ treat, example.data, condition1 = 0, condition2 = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us how noise is different than bias.\n",
    "\n",
    "**Noise** is when a study's answer differs from the truth by random chance, even though the study's answer centers around the correct answer *on average*. That is, if we did the study many times, the right answers would center around the turth.\n",
    "\n",
    "#### High Noise and No Bias\n",
    "\n",
    "For example, an archer with *high noise* but *no bias* would look like this:\n",
    "\n",
    "<img src=\"lowbias_highnoise.png\" width=\"20%\" height=\"auto\">\n",
    "\n",
    "Metaphorically, this is like an experiment with a small sample size (a concept we'll get to in a moment). The answer is right *on average*, but bounces around.\n",
    "\n",
    "#### Low Noise and No Bias\n",
    "\n",
    "Experiments that are *more precise* have answers that are closer to the right answer on average:\n",
    "\n",
    "<img src=\"lowbias_lownoise.png\" width=\"20%\" height=\"auto\">\n",
    "\n",
    "This is obviously even better!\n",
    "\n",
    "### How much noise does my experiment have?\n",
    "\n",
    "But how do we know if my experiment looks like <img src=\"lowbias_highnoise.png\" width=\"15%\" height=\"auto\"> or like\n",
    "<img src=\"lowbias_lownoise.png\" width=\"15%\" height=\"auto\" /> this? That is, how much noise it has?\n",
    "\n",
    "More importantly, when I analyze an experiment, how can I know if the answer I got is just due to noise, or if there really is an effect?\n",
    "\n",
    "To understand this, we need to learn the concept of a **standard error**, which is a measure of far off from the truth an experiment's answer usually is. I.e., the average distance between the center of the target (the truth) and where the archer's arrows actually hit (the estimates we get in particular experiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For today's example, we'll use data from a study by <a href=\"https://www.journals.uchicago.edu/doi/pdf/10.1086/714923\" target=\"_blank\">Grose et al.</a> on social lobbying.\n",
    "\n",
    "In this study, the authors worked with a lobbying firm that was lobbying a state legislature on behalf of an education organization to increase education spending in the state. The authors did the experiment to test their theory that what they call *social lobbying*---taking a legislator to a social location such as a bar or restaurant---is more effective than lobbying legislators in their offices. To test this, they randomly assigned legislators to three groups:\n",
    "- a control group that wasn't lobbied\n",
    "- a social lobbying group that was asked to go to a restaurant or bar and talk about the bill\n",
    "- an office lobbying group that was asked to talk about the bill in their offices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data <- read.csv('ps3_lobbying.csv')\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick rundown of what each column means:\n",
    "\n",
    "- `caseid`: Number that identifies each legislator/district\n",
    "- `supportgroup`: This is the *outcome*. It is a measure of whether the legislator agreed to list their name publicly as a \"sponsor\" of the bill.\n",
    "- `treat`: This is the *treatment*. It has several possible values:\n",
    "    - `\"control\"`: the office received no contact from the lobbyist\n",
    "    - `\"officelobby\"`: the legislator was asked to meet to discuss the bill in their office\n",
    "    - `\"sociallobby\"`: the legislator was asked to meet to discuss the bill at a social location (a restaurant or bar)\n",
    "- `ally`: The authors thought that social lobbying might be especially effective among legislators who had supported the group's priorities in the past. To measure this, they asked the lobbyist: \"In your opinion, how well does the phrase ‘ally of the interest group’ describe the legislator?\" This is therefore the lobbyists' rating of whether the legislator is an ally of the interest group (values 0, 1/3, 2/3, and 1).\n",
    "- `female` : legislator gender, 1 = legislator is female; 0 = not\n",
    "\n",
    "Again, here is what `treat` looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(data$treat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we know how to look at the results. We can use the `difference_in_means()` function we learned last time to see how big of a difference the researchers actually saw between the two experimental conditions in terms of how often the legislators supported the education organization's bill in the legislature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_means(supportgroup ~ treat, data, condition1 = 'control', condition2 = 'sociallobby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big of a difference might we see by chance?\n",
    "\n",
    "This is a pretty small experiment, though. Might we see a difference this big by chance?\n",
    "\n",
    "Let's for the moment assume that the lobbying actually had no effect, so all the potential outcomes are the same: it doesn't matter whether legislators were assigned to the control group, office lobbying group, or social lobbying group. That means we've already seen all the potential outcomes! And it means that the *true average treatment effect*, we are assuming, is zero.\n",
    "\n",
    "In this world, we can examine how the *estimate* bounces around due to simple randomness---the luck of which legislators happen to get put into the treatment group; i.e., noise.\n",
    "\n",
    "**You don't need to be able to write the code below on your own, but I do want to you to understand what it does.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to re-randomize, or \"shuffle\", the treatment variable\n",
    "re.randomize <- function(input.data) {\n",
    "    input.data$treat <- sample(input.data$treat)\n",
    "    return(input.data)\n",
    "}\n",
    "\n",
    "head(re.randomize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(re.randomize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(re.randomize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the effect of social lobbying\n",
    "compute.office.lobby.effect <- function(input.data) {\n",
    "    estimate <- difference_in_means(supportgroup ~ treat, input.data,\n",
    "                                    condition1 = 'control', condition2 = 'sociallobby')$coefficients\n",
    "    return(as.numeric(estimate))\n",
    "}\n",
    "\n",
    "actual.estimate <- compute.office.lobby.effect(data)\n",
    "actual.estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute.office.lobby.effect(re.randomize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute.office.lobby.effect(re.randomize(data))\n",
    "compute.office.lobby.effect(re.randomize(data))\n",
    "compute.office.lobby.effect(re.randomize(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer bounces around every time we run this function. This shows you why we need to worry about noise. If we just took this data, randomly shuffled it into groups, and then looked at the outcomes, having done nothing, we would still see some positive estimates even though putting legislators in the \"treatment\" group obviously did nothing in these simulations.\n",
    "\n",
    "Is this what happened in our actual study?? Does our treatment have no effect on the outcomes, we just happened to randomize legislators likely to sponsor the bill anyway into the treatment group??\n",
    "\n",
    "Let's simulate 10,000 example experiments and see how big these estimates might get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulations <- replicate(10000, compute.office.lobby.effect(re.randomize(data)))\n",
    "head(simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "qplot(simulations, binwidth = .025) + geom_vline(xintercept = actual.estimate, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spread or width of this distribution shows you how large estimates are that we would see by chance.\n",
    "\n",
    "We measure the spread of this distribution using the standard error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd(simulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good News: `difference_in_means()` tells you the standard error\n",
    "\n",
    "You don't need to do this! The `difference_in_means()` function tells us the standard error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "social.dim <- difference_in_means(supportgroup ~ treat, data, condition1 = 'control', condition2 = 'sociallobby')\n",
    "social.dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error is printed under the `Std. Error` heading. In this case, it's 0.076756."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is the standard error relative to the estimte?\n",
    "0.11617 / 0.076756 # divide estimate / standard.error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a _t_-statistic. It's the ratio of the estimate and the standard error:\n",
    "\n",
    "$t = \\frac{\\text{Estimate}}{\\text{Standard Error}}$\n",
    "\n",
    "And `difference_in_means()` tells us this, too! (See 1.513 above under `t value`.)\n",
    "\n",
    "**The _t_-statistic is a way to measure how likely it is that an estimate of the size we saw would arise by chance even if the treatment had no effect.**\n",
    "\n",
    "By convention, **we call an estimate _statistically significant_ if the _t_-statistic is larger than 1.96 (either lower than -1.96 or larger than 1.96).** Smaller than that (closer to zero), and we generally conclude that our estimate could have arisen by chance. We'll say more about this in a future lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula for the Standard Error\n",
    "\n",
    "You won't need to memorize this, but it turns out there is a formula that tells us what a standard error will be in an experiment.\n",
    "\n",
    "Suppose we are comparing groups 1 and 2. Let:\n",
    "\n",
    "- $s_1$ be the standard deviation of group 1\n",
    "- $n_1$ be the sample size of group 1\n",
    "- $s_2$ be the standard deviation of group 2\n",
    "- $n_2$ be the sample size of group 2\n",
    "\n",
    "Then this is the formula for the standard error in an experiment:\n",
    "\n",
    "$ \\text{Standard Error} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} $\n",
    "\n",
    "When the standard deviations are the same (so $s = s_1 = s_2$), this simplifes to:\n",
    "\n",
    "$ \\text{Standard Error} = s \\sqrt{(\\frac{1}{n_1} + \\frac{1}{n_2})} $\n",
    "\n",
    "And, when the two groups are *also* the same size (let $N = n_1 + n_2$ if $n_1 = n_2$), this simplifies further to:\n",
    "\n",
    "$ \\text{Standard Error} = 2 s \\sqrt{\\frac{1}{N}} $\n",
    "\n",
    "What does this mean? For example:\n",
    "\n",
    "- If standard deviation of the outcome variable doubles, the standard error doubles.\n",
    "- If the sample size of a study is cut in half, the standard error increases by a factor of $\\sqrt{2} \\approx 1.41$. (E.g., if the standard error were $X$ before the sample size were cut in half, it would be $X \\sqrt{2}$ after the sample size were cut in half.)\n",
    "- If the sample size of the study doubles, the standard error decreases by a factor of $\\sqrt{\\frac{1}{2}} \\approx .707$. (E.g., if the standard error were $X$ before the sample size were doubled, it would be $\\frac{X}{\\sqrt{2}}$ after the sample size were doubled.)\n",
    "- In order to make the standard error half the size, the sample size must increase by a factor of 4 (i.e., quadruple).\n",
    "\n",
    "What I want you to remember:\n",
    "\n",
    "- The standard error goes up with the standard deviation or spread of the outcome variable.\n",
    "- The standard error goes down as the sample size goes up.\n",
    "- Although an experiment is more precise (i.e., the standard error is smaller) when the sample size is larger, it follows a square root, so we have to quadruple the sample size to cut the standard error in half (since $\\sqrt{\\frac{1}{4}} = \\frac{1}{2}$). Likewise, in order to make the standard error of the treatment effect $\\frac{1}{x}$ the size, the study needs to be $x^2$ times larger.\n",
    "- In an experiment with a fixed *overall* sample size, the more similar in size that two groups are, the more precise the experiment will be. The most precise experiment at a fixed sample size will assign the treatment and control groups with equal probabilities (i.e., 50\\% each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing New Terms/Concepts\n",
    "\n",
    "- **True Average Treatment Effect**: If we could see all the potential outcomes, the actual truth about what the causal effect of a treatment is.\n",
    "- **Estimate**: From a particular study run a particular time, our best guess of what the true average treatment effect is.\n",
    "- **Bias**: When a study's estimates are systematically wrong in a particular direction; e.g., because of omitted variable bias. Experiments have no bias. If a study design is biased, it would be wrong even if its sample size were infinitely large.\n",
    "- **Noise**: Because of random chance, a study's estimate differs from the truth, even though it is on average correct. If a study had an infinitely large sample size, it would have no noise.\n",
    "- **Standard Error**: A way of measuring *how much* a study's estimate will differ from the truth (and between different runs of the same experiment) because of random chance. I.e., a measure of how much noise there is in an experiment.\n",
    "- **t-statistic**: Defined as the estimate divided by the standard error. Gives an indication of how likely a study's result is to have arisen by chance. (More soon on how to use this.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
