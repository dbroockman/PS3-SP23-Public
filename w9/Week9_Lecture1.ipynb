{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1001354",
   "metadata": {},
   "source": [
    "# POLSCI 3\n",
    "\n",
    "## Week 9, Lecture Notebook 1: Describing populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593bf32",
   "metadata": {},
   "source": [
    "## Switching gears to part 2 of PS 3: Describing populations\n",
    "\n",
    "For the first half of the semester, we've been interested in **causal inference**. This is one of the major goals of statistics: understanding the causal effect of treatments. That is, we have a sample of people and we want to understand treatment effects. As we discussed, this is difficult because we can't see the treatment effect for any individual, but experiments allow us to estimate treatment effects for groups of people by comparing average outcomes in the treatment and control groups:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Person</th>\n",
    "    <th>Constituent</th>\n",
    "    <th>Donor</th>\n",
    "    <th>Treatment Effect</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Sally</td>\n",
    "    <td>1</td>\n",
    "    <td>?</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wei</td>\n",
    "    <td>0</td>\n",
    "    <td>?</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Jose</td>\n",
    "    <td>?</td>\n",
    "    <td>1</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Michelle</td>\n",
    "    <td>?</td>\n",
    "    <td>1</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>Average</b></td>\n",
    "      <td><b>0.5</b></td>\n",
    "    <td><b>1</b></td>\n",
    "      <td><b>0.5</b></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "In this case, we were interested in learning about the average treatment effect, even though we couldn't see it directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfeae7",
   "metadata": {},
   "source": [
    "Next, we're going to pivot to learning about a second major goal of statisitcs: **describing populations**. That is, describing big populations using data from only a few members of that population. For example, when we conduct political polls, we survey only a few hundred people and are able to make inferences about the entire population:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>Person</th>\n",
    "    <th>Supports Biden?</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Sally</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wei</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Jose</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Michelle</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan=\"2\">....</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>158,383,403rd voter</td>\n",
    "    <td>?</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td><b>Average</b></td>\n",
    "      <td><b>0.522</b></td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "In this case, we want to know the average support for Biden in the entire population in advance of the election, but we can't survey everyone ahead of time. Instead, we infer the average in the population from those that answer our survey.\n",
    "\n",
    "In both cases, we are trying to infer information we are missing, but what we are trying to infer is different. Before, with causal inference, we were trying to learn about potential outcomes we couldn't see about the individuals we have in our data. Now, with describing populations, we're trying to make inferences about individuals in populations that *aren't in our data* based on only a small subset who are in our data.\n",
    "\n",
    "The good news is that we get to re-use almost all the concepts we learned before, just in this new context. This will include random assignment, bias, standard errors, and confidence intervals!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bec93",
   "metadata": {},
   "source": [
    "## The importance of random sampling: a story\n",
    "\n",
    "We already learned about the importance of random assignment when conducting experiments. Randomness is also important when making inferences about populations. A famous story from history illustrates why.\n",
    "\n",
    "Ahead of the 1936 Presidential Election, the _Literary Digset_, a famous magazine, planned to predict the results of the upcoming election. They thought that to predict the election most accurately, all they needed was the largest sample size possible. So _Literary Digest_ sent surveys to people who subscribed to their magazine and other magazines and who were on club membership lists, etc. -- basically to as many people as they could get their hands on. The finally ended up hearing from **2.2 million people**. Now that's _big data_!\n",
    "\n",
    "But, as we learned with experiments, there's a difference between bias and noise. A big sample size will reduce **noise** a lot, but there might still be a lot of **bias**. Remember this picture?\n",
    "\n",
    "<img src=\"highbias_lownoise.png\" width=\"20%\" height=\"auto\">\n",
    "\n",
    "This is what a study looks like with low noise but high bias -- the answers wouldn't \"bounce around\" much if we did the same study again; but, no matter how many times we did this same study, the answers would be systematically very far from the truth in a particular direction.\n",
    "\n",
    "This is why \"big data\" isn't always useful (that is, data with huge sample sizes). Big data can get noise (i.e., standard errors) very small or even basically zero. But if we have bias, we'll get an estimate systematically different from the truth.\n",
    "\n",
    "Back to the _Literary Digest_ poll. The _Literary Digest_ poll was an estimate with basically zero **noise**... but _a lot_ of **bias**. They wanted to learn about the general population, but instead they learned mostly about relatively wealthy people -- the kind of people who subscribe to magazines.\n",
    "\n",
    "And they were in fact way off. The _Literary Digest_ predicted a landslide for Alf Landon, who was running against Franklin Delano Roosevelt (FDR). The _Digest_ predicted FDR would get only 43% of the vote:\n",
    "\n",
    "<img src=\"digest.png\" width=\"60%\" height=\"auto\">\n",
    "\n",
    "The actual result was a landslide for Roosevelt of 62%. The _Literary Digest_ was _way_ off!\n",
    "\n",
    "This was because of bias -- no matter how large their sample size was, because their procedure was flawed, their answer would be very far from the truth.\n",
    "\n",
    "### Gallup's poll\n",
    "\n",
    "But George Gallup, founder of the polling organization now known as Gallup, understood the difference between bias and noise. He understood that even a few thousand people would give him a sample size to make the noise of his estimate (i.e., standard error) very small, and that the most important thing was to reduce bias. With a sample of only a few thousand people, he was able to predict the election very closely. He predicted FDR would get 56% of the vote, fairly close to the total of 62%. Not bad for one of the first ever polls! In other words, his poll's noise was pretty low, as was its bias:\n",
    "\n",
    "<img src=\"lowbias_lownoise.png\" width=\"20%\" height=\"auto\">\n",
    "\n",
    "That is, yes, if Gallup did his poll again, he might have gotten a slightly different answer (because of noise)---but because Gallup asked a group of people that looked more like the country at large, his answer had less bias and so was more accurate. We call how much a sample looks like a population its **representativeness**.\n",
    "\n",
    "(The _Literaty Digest_ later shut down, partly in disgrace from getting the result so wrong!)\n",
    "\n",
    "This helps illustrate a point we learned when we were talking about inferring causality: don't just focus on getting _more_ data, focus on getting the _right kind_ of data. To infer causality, we usually need some kind of experiment with _random assignment_ to treatment. And to make inferences about public opinion, we usually need _random sampling_ into the survey to guarantee a sample that is **representative** of the population we want to learn about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948e29f",
   "metadata": {},
   "source": [
    "## Sampling bias and random sampling\n",
    "\n",
    "When we conduct descriptive statistics, we look at a **sample** (e.g., respondents to a survey) and try to make inferences about a **population** (e.g., the American public).\n",
    "\n",
    "The _Literary Digest_ was so wrong because their survey had _sampling bias_, which is the bias arises when the people who are represented in a **sample** are not **representative** of (i.e., are systematically different than) the **population** they are meant to describe. There are usually two reasons why sampling bias arises:\n",
    "\n",
    "- **Sample selection bias**: The kind of people selected to be in the sample are systematically different than the population. (E.g., _Literary Digest_ selected people who were subscribers to its magazine and members of high-income clubs, who were systematically wealthier than the population. This produced sample selection bias.)\n",
    "- **Non-response bias** (relevant for surveys): The kind of people who agree to respond to a survey are systematically different than population sampled. (E.g., if more educated people are more likely to answer surveys, even if we begin with a random sample of people we _ask_ to take a survey, we might end up with an unrepresentative sample of people who _complete_ the survey.)\n",
    "\n",
    "Let's see how this happens in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "library(dplyr)\n",
    "\n",
    "# You don't need to learn how to code this part.\n",
    "# Make fake datasets\n",
    "identifies.as.male <- data.frame(gender.male = 1,\n",
    "                                 supports.tax.increase = c(0,0,0,0,0,0,0,0,0,1))\n",
    "does.not.identify.as.male <- data.frame(gender.male = 0,\n",
    "                                        supports.tax.increase = c(1,1,1,1,1,1,1,1,1,0))\n",
    "\n",
    "\n",
    "\n",
    "# One overall dataset. This is our population we want to learn about.\n",
    "everyone <- rbind(identifies.as.male, does.not.identify.as.male)\n",
    "everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ed6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(everyone$supports.tax.increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50da980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I take a true random sample, I get an answer that resembles the overall population.\n",
    "a.random.sample <- sample_n(everyone, 10)\n",
    "mean(a.random.sample$supports.tax.increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I take a true random sample, I get an answer that resembles the overall population.\n",
    "a.random.sample <- sample_n(everyone, 10)\n",
    "mean(a.random.sample$supports.tax.increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.random.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50555db3",
   "metadata": {},
   "source": [
    "But a non-random sample will give us an answer very far from the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b440ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "non.random.sample <- rbind(sample_n(identifies.as.male, 9),\n",
    "                           sample_n(does.not.identify.as.male, 1))\n",
    "mean(non.random.sample$supports.tax.increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non.random.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b274ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non.random.sample <- rbind(sample_n(identifies.as.male, 9),\n",
    "                           sample_n(does.not.identify.as.male, 1))\n",
    "mean(non.random.sample$supports.tax.increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9ec52",
   "metadata": {},
   "source": [
    "## Polls are broken; weights help... kind of\n",
    "\n",
    "By far the most common application of describing populations is surveys intended to describe opinion in the general population.\n",
    "\n",
    "One common way people assess how accurate polls are is to see how well polls that ask people how they will vote in an election predict an election result. In the golden era of polling, sampling bias was small: polls predicted elections fairly well. That... isn't as true any more. Why?\n",
    "\n",
    "- Sample selection bias has gotten worse. In the mid 20th century, most households had a land line. Pollsters used \"random digit dialing\" to randomly select households. Now, how do we select a random sample of Americans in the first place? It is difficult. ANES: randomly select addresses, fly out surveyors and knock on doors!\n",
    "- Non-response bias has gotten worse: the 6% of people who answer surveys are probably pretty weird.\n",
    "<img src=\"rrates.webp\" width=\"30%\" height=\"auto\">\n",
    "\n",
    "To try to deal with these problems, we apply _weights_ to survey data. Weights are intended to make our sample looks more like the population we are interested in; it is a statistical way to make our sample more representative.\n",
    "\n",
    "We calculate a weight by _comparing our sample to the population_ and then _counting observations more often_ to \"stand in\" for the people we are \"missing\" from groups who are under-represented. For example, we might calculate weights like this:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>% in population</th>\n",
    "    <th>% in sample</th>\n",
    "    <th>Weight</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Identify as Male</td>\n",
    "    <td>50%</td>\n",
    "    <td>75%</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Do not Identify as Male</td>\n",
    "    <td>50%</td>\n",
    "    <td>25%</td>\n",
    "    <td>3</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Then, when we analyze the data, we use the function you learned last week: `weighted.mean()`! This counts observations more often when it has a higher weight. E.g., an observation with a weight of 2 would be counted twice.\n",
    "\n",
    "As with observational data for inferring causality, though, the possibility of bias remains. We have to make a similar assumption as we did when analyzing observational data for causal inference: that there are no _omitted variables_ that both affect whatever outcome we're looking at in the survey (e.g., support for FDR) and being selected for or answering the survey (e.g., being wealthy).\n",
    "\n",
    "Unfortunately, there's no such thing as a \"gold standard\" experiment for survey data that solves the omitted variable bias problem for survey data. Nowadays, all surveys contain the possibility for omitted variable bias. We just do our best to try to adjust for as many omitted variables as we can using weighting.\n",
    "\n",
    "Let's see how this works in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3988b",
   "metadata": {},
   "source": [
    "## This week's data: Cooperative Election Survey\n",
    "\n",
    "One of the largest surveys that political scientists conduct is called the Cooperative Election Survey. It surveys tens of thousands of people before every election.\n",
    "\n",
    "The surveys are conducted online by the firm YouGov. YouGov doesn't randomly sample Americans; instead, YouGov invites as many people as it can, then weights the data to try to match the population on the variables it can observe (e.g., race, gender, education, income, and lots more). (Funnily enough, this is in some ways similar to what Gallup did in 1936, before random sampling was fully understood.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752295f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data <- read.csv(\"ps3_cces2020_pre.csv\")\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fec234",
   "metadata": {},
   "source": [
    "Here's what the variables mean:\n",
    "\n",
    "- `vvweight`: Survey weight\n",
    "- `college`: Whether the person went to college (`0` = no; `1` = yes)\n",
    "- `medicare_expand`: Whether the person favors expanding Medicare (medical program for people over 65)  (`0` = no; `1` = yes)\n",
    "- `repeal_aca`: Whether the person favors repealing the Affordable Care Act (\"Obamacare\") (`0` = no; `1` = yes)\n",
    "- `regulate_carbon`: Whether the person favors regulations on carbon emissions (`0` = no; `1` = yes)\n",
    "- `prefertrump`: Whether the person prefers Trump to win the 2020 Presidential election (`0` = no; `1` = yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2e7cd",
   "metadata": {},
   "source": [
    "### Using weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689d4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many people went to college in this dataset?\n",
    "mean(data$college)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ef750",
   "metadata": {},
   "source": [
    "Oh wow, that's too many.\n",
    "\n",
    "In reality, only about 32% of US adults have gone to college.\n",
    "\n",
    "An example of why this bias in the sample might matter: Voters with college degrees were less supportive of Trump, so a dataset that _over-represents_ college-educated voters will probably be too anti-Trump. And, indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of the prefer Trump variable.\n",
    "mean(data$prefertrump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae3023",
   "metadata": {},
   "source": [
    "We're probably underestimating Trump support (he won 46.9% of the vote).\n",
    "\n",
    "We can try to correct for this by using weights! We use the `weighted.mean()` variable with the command `weighted.mean(data$outcome, data$weight)`, replacing `data$outcome` with the outcome variable name and `data$weight` with the survey weight. In this particular dataset, the survey weight variable is stored in the `vvweight` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted.mean(data$prefertrump, data$vvweight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68832472",
   "metadata": {},
   "source": [
    "Now, we see Trump support is higher among registered voters once we weight. But we still have the answer somewhat wrong.\n",
    "\n",
    "This is why the polls were off in 2016 and 2020 -- the people who do polls can be different!\n",
    "\n",
    "But this doesn't mean this is the right answer necessarily. The sample still might be unrepresentative even with weights. Weights try to make things better, but there can still be remaining omitted variable bias if we haven't weighted on all of the variables that determine whether someone responds to the survey."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
